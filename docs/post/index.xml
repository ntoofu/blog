<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on ntoofu</title>
    <link>https://ntoofu.github.io/blog/post/</link>
    <description>Recent content in Posts on ntoofu</description>
    <generator>Hugo -- gohugo.io</generator>
    <copyright>&amp;copy; 2018 ntoofu</copyright>
    <lastBuildDate>Fri, 04 Jan 2019 00:17:21 +0900</lastBuildDate><atom:link href="https://ntoofu.github.io/blog/post/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Topotestsを利用してルーティングについて勉強する</title>
      <link>https://ntoofu.github.io/blog/post/how-to-learn-routing-with-topotests/</link>
      <pubDate>Thu, 23 Jun 2022 00:00:00 +0000</pubDate>
      
      <guid>https://ntoofu.github.io/blog/post/how-to-learn-routing-with-topotests/</guid>
      <description>ルーティングについての社内輪読会の発表のために、デモ環境の構築を Topotests で実施し、とても便利だったので紹介する。
背景 デモ環境について ルーティングプロトコルについて学ぶ中で、実際に動かしてその動作を確かめたいと考えることもあると思う。 実際に設定をして期待したとおりに経路情報が伝播するのか、障害時に代替の経路に切り替わってくれるのか、やり取りされるパケットの中身がどのようになっているのかなど、見たい部分もたくさんあるだろう。
しかし、複雑なルーティングプロトコルの動きを見るためには、ルーターが複数台(それも2,3台ではなく5台以上)欲しくなってくるのはほぼ間違いない。 これらを物理機器として準備するのは場合によっては難しく、x86などの一般的なコンピューター上でソフトウェアとして動作するもの（いわゆる仮想ルーター）を使うほうが、ハードルが低い。
ただし、ソフトウェア的に環境を作る場合、ネットワークのトポロジを模擬する部分に工夫が必要になってくる。 例えば、ルーター1つごとに仮想マシンを立てて複数の仮想NICを割り当てたり、コンテナなどのLinuxのNamespaceの機能とvethやbridgeなどの仮想ネットワーク機能を上手く使うといった具合だ。 そうなってくると、結局ソフトウェア的に環境を作るのも、相応の手間が必要になってしまう。
Topotests こうした問題を解消できる一つの方法として、Topotests を利用する方法がある。
Topotestsは、FRRouting(FRR)というLinux/Unixで動作するオープンソースのルーティングプロトコルスイートのプロジェクトの中で、そのテストを行うためのライブラリである。 前述のLinuxのNamespaceの機能と仮想ネットワーク機能を利用して、ネットワークトポロジを再現し、ビルドしたFRRを動作させ、期待する動作をするか（正しくルートが学習されるか、メモリリークがないか等）というテストを、Topotestsによって実現している。 （トポロジ再現のキモは micronet という部分に集約されており、見ての通りiproute2コマンドを組み立てて実行している。）
Topotestsを用いたテストのために準備するトポロジの定義は、Pythonのコードの中に書くこともできるが、トポロジを定義したJSONファイルを用意することもできる。 多くのルーターとその間のリンクを定義すること考えると、Pythonのコードとして定義したほうがループなどの制御構文が利用できて簡単にも思えるが、ルーティング用のデーモンに渡す設定ファイルを別途準備する必要もあるため、意外と面倒になる。 JSONファイルで準備する場合、それらの設定まで同じJSONの中で表現できるため、総じて見るとよりシンプルになる。
また、ドキュメントの中にも書かれている通り、--topology-only オプションを指定すれば、ネットワークトポロジの再現とFRRを動作させるところまで実行しつつ、テストは実施しないようにもできる。
したがって、Topotestsを使えば、JSONで定義したネットワークトポロジを、FRRとLinuxのNamespaceと仮想ネットワーク機能を用いて再現することができるということになる。
手順 コンテナの用意 FRR自体のテストのためではなく、ルーティングの学習用に任意のトポロジを再現するためにTopotestsを使うという観点で考えると、現時点 では、UbuntuベースのDockerイメージをビルドして用いるのが良い。 (Dockerhubの frrouting/frr イメージは、pytest等のライブラリが入っていないため、ドキュメントに書かれているような方法でTopotestsを実行できないため。 frrouting/topotests というイメージもあるが、都度FRRをソースコードからビルドすることを前提としており、FRR自体のテストを考えていないのであればむしろ使いにくいと思う。)
ビルドはFRRのリポジトリのルートでdocker buildすれば良いだけ。
git clone https://github.com/FRRouting/frr cd frr docker build -t frr -f docker/ubuntu20-ci/Dockerfile . トポロジとテストの準備 トポロジを定義したJSONファイルを用意する。 ドキュメントや、Gitリポジトリ内のTopotestsを利用したテストで使っているJSONファイルを参考にすると良い。
また、そのトポロジを読み込むテストを用意する。 exampleの中にあるものを参考に作ると手っ取り早い。 テストケースが無いと、pytest経由で呼び出したときに実行するテストがないとみなされ何もせずに終了してしまうので、中身は空でも良いので test_ で始まる関数を1つは用意する。
例えば、こんな感じ。
実行 準備ができたら、用意したファイルをマウントしつつ先にビルドしたコンテナを実行する。
docker run -v $PWD:/home/frr/frr/tests/topotests/mytest:ro --privileged --rm -it frr /bin/bash あとは、コンテナ内で</description>
    </item>
    
    <item>
      <title>Gentoo Linux で (Portageで) パッケージビルドサーバーを使う</title>
      <link>https://ntoofu.github.io/blog/post/gentoo-binpkg/</link>
      <pubDate>Sun, 01 May 2022 00:00:00 +0000</pubDate>
      
      <guid>https://ntoofu.github.io/blog/post/gentoo-binpkg/</guid>
      <description>Gentoo Linux でPortageを利用してソフトウェアをインストールする場合、一般的にはソースコードを取得しビルドすることになるが、消費リソースや依存関係の問題で苦労することも多い。その問題を緩和する方法として、Portageがサポートしている Binary Package のビルドサーバー構築を行った。その大まかな手順を書き残しておく。
背景 Portageの弱点 Gentoo Linuxでは、標準でPortageというパッケージマネージャを利用する。 その大きな特徴として、パッケージインストール時に、事前にコンパイル・ビルドされたバイナリではなくソースコードを取得してきて、手元の環境でビルドすることが挙げられる。 これにより、細かな粒度で機能の有効化・無効化を調整したり、コンパイラによる最適化の恩恵を最大限に受けることができる。
一方で、手元でビルドする以上避けられない問題もある。 まず、パッケージインストール時に消費するリソースが大きいことが挙げられる。 ビルドする以上当然のことではあるが、Webブラウザのような巨大なソフトウェアのビルドには多くのCPU,メモリ,ストレージのリソースを消費する。 例えばFirefoxをビルドしようと考えると、Rustがビルド時に必要となるが、このインストールに際しては7GiB以上のディスクの空きスペースが要求される。
また、依存関係の解決にも苦労することがある。 目的のパッケージをインストールするにあたって、そのビルドに必要となるパッケージもインストールしなければならないため、通常より依存関係が複雑になりやすい。 パッケージによっては同時に環境内にインストールされることを禁じられている組み合わせもあり、これによって思うようにパッケージ更新が進まないことも多い。 特に、しばらくシステムの更新を怠っていると、同時に多くのパッケージの更新を行う必要が出てきてしまい、こうした問題に当たりやすくなる。
個人的な問題として、長い間放置していたラップトップにインストールしていたGentooのシステムを更新しようとした際に、まさにこれらの問題が大きな妨げとなった。 数世代前のマシンのスペックで大量のパッケージをソースコードからビルドすることは大きな負担であったし、依存関係を紐解いてすべてのパッケージのバージョンを新しくしていくことは現実的でないように思えた。
Binary Package こうした問題に対する解決策の一つとして、Binary Packageが挙げられる。
Binary Packageは、パッケージをビルドした後のファイルをまとめたもので、これを利用してパッケージインストールをする場合は、ソースコードをコンパイルする工程が不要となる。 多くのLinuxディスリビューションにおいて、パッケージのインストール時には配布されているコンパイル済みのバイナリを取得してくるのによく似ているが、Portageの場合Binary Packageが配布されているわけではないため、自身でBinary Packageのビルドを行わねばならない。 したがって、実際の活用方法は、あるホストでビルドしたものを同じ構成の別ホストに流用したり、バックアップとしてビルド済みのものを退避しておく、といったことが想定されているようだ。
個人的に抱えていた古いラップトップ上のシステム更新に際しても、この方法は活用できそうだ。ラップトップ上のシステムと同等の環境を、より性能の高い別のホスト上に準備して、そちらでパッケージのビルドをすべて行ってしまい、その成果物をBinary Packageとしてラップトップに対して配布すれば良い。
distccとの比較 また、別の案としてdistccによる分散コンパイルを行う方法も考えられる。
Portageからはdistccという分散コンパイラを呼び出すことができ、コンパイルの処理の一部をリモートホストに任せることができる。ローカルホストとリモートホストでgccのバージョンを揃える必要があるなど、いくつかの注意点はあるが、比較的手軽にローカルホストの負荷軽減を期待できる方法である。
ただし、ビルドそのものはローカルホストで実行することになるため、ビルド時に依存するパッケージのインストールは必要になるし、オブジェクトファイルのリンク処理などはローカルホストで実施することになる。そのため、今回はdistccを利用することはしていない。
SYSROOT Binary Packageをビルドする際には、バイナリをビルドする環境と実行する環境が異なる点に注意する必要がある。 CPUアーキテクチャのレベルで異なればクロスコンパイルをすることになるが、例えば同じx86-64系でマイクロアーキテクチャが異なる程度であれば、同じツールチェインを用いつつコンパイルオプションを変えることになる。 今回はマイクロアーキテクチャは異なるがアーキテクチャはx86-64で同一というケースに絞って考える。
基本的には実行環境に最適化してコンパイルができれば良いのだが、ビルド時に必要なパッケージについてはビルド環境で動作することもあるため、Portageのビルド時の設定を指定する make.conf で一律に実行環境向けの設定を書くわけにもいかない。 ビルド環境と実行環境で共通して動作するレベルまで最適化を控えるようにするのも手だが、せっかくGentooを使っているのだから、最適化したバイナリが得られるように工夫することを考える。
実は、まさにこうした事態をPortageは想定しており、 ROOT SYSROOT PORTAGE_CONFIGROOT 環境変数の指定によって挙動を細かく制御できるようになっている。 詳細はmanにも記載がある。 これを利用することで、別のホストに最適化したBinary Packageをビルドすることが可能となる。
手順 ビルドサーバーの準備 試行錯誤することが予想されたこともあって、環境の再作成が容易なようにDockerコンテナを使った。 適当に用意したそこそこのスペックのサーバー上で gentoo/portage イメージとvolumeを共有する形で gentoo/stage3 のイメージを動かし、これをビルド環境として docker exec にて作業することにした。 （イメージの詳細については https://github.com/gentoo/gentoo-docker-images を参照）
ビルド環境内には /alt というディレクトリを作って、Binary Package向けの各種設定や成果物の配置先として使う。 これは、前述の ROOT SYSROOT PORTAGE_CONFIGROOT で指定する。 例えば ROOT=/alt SYSROOT=/ PORTAGE_CONFIGROOT=/alt のようにすることで、/alt/etc/portage/make.</description>
    </item>
    
    <item>
      <title>DockerコンテナイメージをvSphere仮想マシンイメージに変換する</title>
      <link>https://ntoofu.github.io/blog/post/docker-to-vsphere/</link>
      <pubDate>Sun, 19 Sep 2021 00:00:00 +0000</pubDate>
      
      <guid>https://ntoofu.github.io/blog/post/docker-to-vsphere/</guid>
      <description>Dockerでコンテナイメージをビルドしてから変換することで、仮想マシンイメージを作成することができたので、その方法を説明する。
背景 ソフトウェア動作環境として、仮想マシンよりもコンテナを利用することのほうが最近は一般的になっており、 技術の流行に従えば、あえてコンテナを仮想マシンに変換するようなことは必要ないようにも思われるかもしれない。 確かに、DevOpsのサイクルを高速に回す上で、コンテナの軽量さはメリットであるし、 Kubernetesのようにデプロイを含めたライフサイクルを管理するツールがあるため、 Continuous Deploymentを導入しやすいという点も大きい。 （このあたりは、その軽量さも相まって仮想マシンに比べImmutable Infrastructureに向いていることが影響していると思う。）
しかしながら、以下のような理由により、仮想マシンを使い続ける場面も多くある。
ネットワークの要件により、コンテナとしてデプロイすることが難しいことがある 物理機器とのL2での疎通性が必要な場合など CNIプラグインによりコンテナでも理屈の上では実現できることも多いが、ファイアウォールを適用することに難があったり、パフォーマンス上の懸念も残るなど、直ちに実用できるとは言い難い 既存の資産の有効活用 仮想マシンを前提に出来上がった運用があると、移行コストがかさむため、直ちにコンテナ化するとは限らない とはいえ、仮想マシンのイメージのビルドは、コンテナのイメージのビルドに比較すると、通常手間がかかるのも事実である。 仮想マシンをデプロイし、SSHなどを利用してパッケージのインストールなどのプロビジョニングを実施するため、 オーバーヘッドも避けられない上に、そもそも仮想マシンをデプロイする環境がないとビルドできない。
そこで、ビルドを簡略化しようという狙いで、Dockerコンテナイメージをビルドし、 それを仮想マシンのイメージに変換するということをやってみた。
手順 PoC https://github.com/ntoofu/docker-to-vsphere
build.sh を実行し、生成される build/vm.ovf をインポートすればOK （手順の詳細はこのリポジトリの中身を見たほうが良いかもしれない） 内容 alpine などのディストリビューションのイメージをもとに、通常通りDockerfileを作ってコンテナビルドするが、以下の工夫をする Linux kernelをインストールする apk add linux-virt /etc/fstab /boot/grub/grub.cfg をイメージに追加する OpenRCや、関連して必要になるパッケージを追加する openrc busybox-initscripts e2fsprogs システム的に必要なサービスの自動起動設定をする docker build 試していないが、 kaniko などでビルドするほうが（CI環境でdocker buildさせる場合はDocker in Dockerなどの工夫が必要になりがちで、それを避けられるため）都合がよいかもしれない ビルドしたイメージからコンテナを作成し、そのコンテナのファイルをアーカイブファイルとして出力する docker export でアーカイブファイルとしてコンテナ内のファイルを出力できる dd などで空のイメージファイルを作成し、 parted などでパーティションテーブルを作成 イメージファイルに対応するloopbackデバイスを作成し、パーティションにファイルシステムを作成（フォーマット） loopbackデバイスをマウントし、その中にコンテナのアーカイブファイルを展開 後述する理由で、 .dockerenv boot/boot を exclude grub-install により、grub2をインストールする qemu-imgで streamOptimized な vmdk ファイルに変換 OVFファイルを作成する 補足 OpenRCは rc_sys という設定項目があり、未設定の場合、動作環境を検出し、Dockerコンテナ内と判定された場合、一部サービスが起動しなくなる /.</description>
    </item>
    
    <item>
      <title>PackerからzerofreeによりOVAイメージのサイズを小さくする方法</title>
      <link>https://ntoofu.github.io/blog/post/zerofree-via-packer/</link>
      <pubDate>Tue, 24 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://ntoofu.github.io/blog/post/zerofree-via-packer/</guid>
      <description>日頃仮想マシンやハイパーバイザ周辺を見ていることが多く、 仮想マシンイメージの自動ビルドを整備するような機会も少なくない。
仮想マシンのイメージをビルドする際、そのサイズを小さく抑えることは重要で、様々な恩恵がある。 （これは、コンテナの長所の一つにその軽量さが挙げられることからも想像できると思う。）
少々ニッチなケースではあるが、仮想マシンイメージのサイズを縮める部分で工夫を要する状況に遭遇したので、 その時の対処をメモしておく。 なお、ビルドの実行環境はvSphereを前提にしている。 それ以外の環境でこの手順が必要になるケースがあるかどうかは定かでないが、 vSphereに強く依存した手順ではないため、必要になった場合は応用できると思われる。
予備知識 HashiCorp Packer https://www.packer.io/
仮想マシンイメージやコンテナイメージのビルド手順を自動化する際によく使われるソフトウェア。 ビルド環境として各種IaaSやvSphere等仮想化基盤をサポートしている。
OVF/OVA 仮想マシンの仮想ハードウェアとしての仕様を記述するOVF(Open Virtualization Format)というフォーマットがあり、vSphereでも仮想マシンのインポート・エクスポートに使われている。
実際にインポート・エクスポートを行う際は、 OVFファイルに加えてディスクイメージ(vSphereだとVMDK形式)が使用される。 そのため、これらディスクイメージやそのハッシュ値を収めたファイル等とOVFファイルをまとめて TAR形式でアーカイブし、使い勝手を良くした状態で扱うこともあり、このアーカイブファイルをOVAと呼ぶ。
zerofree OVF+ディスクイメージをエクスポートする際、vSphereではディスクイメージ中に存在する NULLブロック (0x00 のみのブロック) の記録を省略し ファイルサイズを小さくするように動く。 （それを示す記述として探した中で一番近いものは このドキュメントの &amp;ldquo;The disk files are stored in a compressed, sparse format.&amp;rdquo; であり、詳細は記載されていないが、 実際の結果などを見る限りNULLブロックの記録が省かれるようである。）
Thin provisioning で仮想ディスクを利用している場合、仮想マシンがブロックを利用するまで実際にはブロックを確保しないため、 未使用の領域はディスクイメージに含まれることはない。 しかしながら、仮想マシン内でファイル作成後に削除した場合など、一度使ったが解放したブロックというものが通常は存在する。 そして、それらは通常再利用され新しいデータで上書きされることになるため、ファイルシステムはわざわざコストをかけてゼロ埋めしない。 結果として、すでに削除したデータが残っているだけの不要なブロックが、エクスポート後のディスクイメージに含まれてしまう。
そのため、仮想マシンのディスクイメージをエクスポートする前に、データを保持していないブロックをゼロ埋めしておくことで、 エクスポート後のサイズを縮めることが可能になる。これを実現するツールが zerofreeコマンドになる。
ただし、利用中のブロックをどう管理しているかはファイルシステムに依存しており、 zerofreeはext2, ext3, ext4しかサポートしていないようなので、その点に注意が必要である。 また、対象のファイルシステムがread-onlyでマウントされていないと利用できないという制限もある。
ここまで、仮想マシンのエクスポートに際しては事前にzerofreeを実行するとイメージのサイズを縮められるという話をしたが、 実際にはもっとスマートな方法が存在する。
fstrim HDDのような記録媒体であれば、前述のようにわざわざブロックをゼロ埋めする必要はなく、そういった処理は通常されない。 しかし、HDDとは記憶の仕組みが異なるSSDにおいては、データを上書きするような場合に、物理的には記憶素子に対し一度初期化を 行ってからデータを書き込むような手順が必要であるらしい。 （それも、ある程度まとまった単位で初期化する必要があり、変更がない部分も書き戻す必要が生じて相応のコストがかかるらしい。） そのせいで、長期間SSDを利用した場合に書き込み速度が低下するようなことが起こりうる。
それに対し、不要になったデータを保持している素子について、予め非同期的に初期化しておけば、 その箇所を再利用する際の初期化の手間を省き、書き込み速度の低下を軽減できると考えられる。 これを実現するために、SSDのような記録媒体に対し、データが不要となったことを通知するための方法が存在する。 ATAのデバイスにおいてはTRIM, SCSIデバイスにおいてはUNMAPと呼ばれる命令がそれである。 （余談: コンシューマにSSDが流行りだした当時、自作PC関連の情報をよく追っていたが、 TRIMの対応に関しては自作PC界隈でもよく取り上げられた話題だったと記憶している。）</description>
    </item>
    
    <item>
      <title>TLSのパケットキャプチャ方法</title>
      <link>https://ntoofu.github.io/blog/post/pcap-for-tls-traffic/</link>
      <pubDate>Sun, 11 Jul 2021 00:00:00 +0000</pubDate>
      
      <guid>https://ntoofu.github.io/blog/post/pcap-for-tls-traffic/</guid>
      <description>先日プロフェッショナルSSL/TLS を読み、 社内勉強会でTLSについて話した。 その際、ブラウザと各Webサイトの間で実際どのようにTLSのハンドシェイクがなされているかを説明するために、 ブラウザのTLS通信をパケットキャプチャしたのだが、うまくキャプチャするのに多少工夫したので、 そのことをこの記事では説明する。
（普段からLinuxデスクトップを利用しているので、Linuxを前提とする。）
背景 パケットキャプチャそのものはブラウザを実行するホスト上で tcpdump などを使えば容易に実行できるが、 ブラウザからのTLS通信をキャプチャする場合、次の問題がある。
暗号化されているため、中身が確認できない 特にTLS 1.3では、1.2以前は平文だったようなハンドシェイク途中のメッセージも暗号化されているものがある TLSのハンドシェイクそのものだけでなく、上位プロトコル(HTTPなど)の内容も確認したい場合は、復号が必須 関心のあるTLSのセッションだけを選択してキャプチャすることが難しい 送信先IPアドレスは特定困難 WebサイトのIPアドレスはDNSラウンドロビン等で複数から選んで使っている可能性があって、変動する サイト内の一部のコンテンツがサブドメインにあったりCDNを利用していて、送信先アドレスがまちまち キャプチャに使うホスト上で、関係ないプログラムも同じポートやプロトコルを使うため、判別が困難 TCP 443宛やTLSプロトコルを使っているという条件では、キャプチャしたいブラウザ以外からの通信が混ざる 方法 本題となるキャプチャ方法だが、ここでは以下の3つについて説明する。
SSLKEYLOGFILEを活用し、パケット復号する方法 netnsを活用し、特定プロセスのみを対象にしてパケットキャプチャする方法 NFLOGを活用し、特定プロセスのみを対象にしてパケットキャプチャする方法 2番目と3番目に関しては、どちらかを選んで使えばよい。 （先日の社内勉強会のためにキャプチャしたときは、1+3の方法で実施した。）
SSLKEYLOGFILEを活用し、パケット復号する方法 SSLKEYLOGFILE で調べると説明しているところがたくさんあるので、細かい手順は省くが、要約すれば
ブラウザを SSLKEYLOGFILE 環境変数にファイルパスを指定した状態で起動する すると SSLKEYLOGFILE で指定されたパスのファイルにTLSセッションごとの鍵に関するパラメタが記録される このファイルをパケットキャプチャと合わせてWiresharkに読み込ませると、TLSセッションを復号してくれる 設定箇所: 編集(Edit) -&amp;gt; 設定(Preferences) -&amp;gt; Protocols -&amp;gt; TLS -&amp;gt; (Pre)-Master-Secret log filename ということになる。TLSクライアント機能を有する任意のソフトウェアで同じ手法が使えるわけではないが、FirefoxやGoogle Chromeなどの主要ブラウザでは使える方法であるらしい。（もちろん、ブラウザのバージョンやビルドオプション等にも依存するとは思うが）
軽く調べた感じだと、以下のような事情になっているらしい。
SSLのライブラリであるNSSには、デバッグ用にTLSセッションを複合するのに必要な鍵情報をSSL key log fileとして吐き出す機能がある SSLKEYLOGFILE 環境変数が設定されているとき、それが示すファイルパスに定められたフォーマットでpremaster secret等を出力する よってNSSをバックエンドにするソフトウェアではこの方法が使えるはず TLSに関するソフトウェアのいくつかが、このSSL key log fileをサポートしている キャプチャしたパケット解析に広く使われるWiresharkでは、このSSL key log fileを使ってパケットの復号をする機能をサポートしている ちなみに、NSSを使うソフトウェアだけでなく、OpenSSLなども同じフォーマットのSSL key logを受け取るコールバック関数がある NSSのように SSLKEYLOGFILE 環境変数をライブラリ側で読み取ってファイルに吐き出すところまではやってくれなさそう netnsを活用し、特定プロセスのみを対象にしてパケットキャプチャする方法 netns ことnetwork namespaceというLinuxの機能を使うことで、 Linuxカーネルのネットワークスタックは仮想的に分離できる。</description>
    </item>
    
    <item>
      <title>CNIプラグインを活用してKubernetesのPodをVLANにつなぐ</title>
      <link>https://ntoofu.github.io/blog/post/k8s-pod-connected-to-vlan-by-bridge-cni/</link>
      <pubDate>Mon, 21 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>https://ntoofu.github.io/blog/post/k8s-pod-connected-to-vlan-by-bridge-cni/</guid>
      <description>この記事は富士通クラウドテクノロジーズ Advent Calendar 2020の21日目の記事です。
TL;DR KubernetesでPodを外部のVLANを利用したネットワークと接続したい場合の一つの方法を検証した Multus CNI と bridge CNI pluginを活用する 都度ノードを自ら設定したりする必要はなく、manifestを適切に与えるだけでPodを好きなVLANに直接(L2レベルでの疎通がある状態で)繋げられるようになる 背景 PodをVLANに繋ぎたい動機 業務ではインフラの管理を中心に担当しているが、IaaSとしてサービス提供している環境（いわばクラウドの裏側）の管理であり、クラウド上のシステムではないため、泥臭いことを沢山する必要がある。例えばDHCPサーバーを構築してホストのアドレスを管理できるようにしたり、ユーザーに提供しているサーバーインスタンスが利用するネットワーク機能（ファイアウォール、ロードバランサー、VPN等）を提供することなどがある。
それら必要なものには仮想マシンとしてデプロイしているものも多くあるが、自動でダウンタイムなく安全に更新できるような構成を取れていないものも残っており、管理上負担になっている。コンポーネントによって手順も異なるため、手がかかっている。
そこで、それらをKubernetesの上に立て直すことで、デプロイメントやアップデートの方法を共通化し、リリースサイクルを高速化していくことが出来ないかを考えている。
しかし、KubernetesではPodのネットワークにはある程度の制約があるため、先に例に上げたようなある種の泥臭いことをKubernetes上でやろうとすると、一筋縄ではいかない。何も考えずにクラスタを構築すれば、PodはFlannelによって作られるオーバーレイネットワークのみに接続していて、DHCPによるアドレス払い出しを行うことも、ネットワーク間のパケット転送に関わることもできない。
そこで、今回はまずIEEE 802.1QのVLANに自由にちょっかいを掛けられるようにする検証をしてみた。
CNIプラグイン コンテナはランタイムの乱立と標準化の中で、コンテナ（より厳密にはLinux network namespace）のネットワークへの接続の実装を柔軟に取り回せるよう、インターフェイスとしてContainer Network Interfaceが定められている。そして、そのインターフェイスに準拠して作られた実装がCNIプラグインである。（CNIについての詳細は過去にも調べて軽くまとめている。）
KubernetesもこのCNIに対応していて、適切に設定することでPodネットワークの実装を切り替えたり出来る。（このあたりはクラスタ構築をする際に設定が必要なため、クラスタ構築をしたことがあれば心当たりがあると思う。）
今回はCNIプラグインとして以下で説明するMultus CNIとbridgeプラグインを使うことで、目的を果たせることを検証していく。
Multus CNI これも過去に少し調べて書いているが、簡単に言えば複数のCNIプラグインを呼び出せるようにするためのCNIプラグインで、コンテナ（Kubernetes的にはPod）の中に複数のネットワークインターフェイスを与えることなどが可能になる。
bridge CNI plugin 標準的なCNIプラグインとして https://github.com/containernetworking/ の下で提供されている。主なところとしては
Linux bridge interfaceを（無ければ）ホストに作成する コンテナが利用するnetns(network namespace)内と、ホストを結ぶLinux veth interfaceを作成する （必要に応じて）IPアドレスをnetns内のインターフェイスに割り当てる 設定によってはこのbridgeを介して外部にSNAT(Masquerade)でIPによる通信が出来るようホストを設定する といったものになっている。直接使うことはあまりないかもしれないが、実は今なにも考えずにflannelを利用すれば(これを使ってインストールした場合)内部的にbridgeプラグインを呼び出している。
更にこのbridgeプラグインは、Linux bridge interfaceの vlan_filtering 利用できるようになっている。余談だが、このPRで実装されたようなので、以前検証した時にはなく、今回自分でプラグインを作成または拡張しようとして、参考にソースコードを読み込んでいて気がついた。
これを上手く使えば、必要なネットワークの定義などをほとんどKubernetesのmanifestに完結させて行うことが出来るはずなので、それを確認していく。
他のプラグイン https://github.com/containernetworking/plugins/ の中には、他にもプラグインがあるのでそれについても少し言及しておく。
vlan: 今回とかなり近いことが可能だが、VLAN IDごとにvlan typeの仮想インターフェイスを作成し、それをコンテナ(厳密にはnetwork namespace)に直接渡してしまう。そのため、同一Node上で複数のPodが同一VLAN IDを利用できない。 ipvlan, macvlan: どちらもvlanと名前に含まれているが、Linuxにおけるipvlan, macvlanタイプの仮想インターフェイスのことを指しており、IEEE 802.1QのVLAN tagを付け外しすることはない。 検証 テスト用ネットワークの準備 ノード対してNICを追加し、untagされていないフレームを転送するよう(トランクポートとなるよう)スイッチを設定 画像はテスト環境に利用していたVMware vSphereのUIからのスクリーンショット</description>
    </item>
    
    <item>
      <title>Eclipse TitanでTTCN-3を試した</title>
      <link>https://ntoofu.github.io/blog/post/eclipse-titan/</link>
      <pubDate>Sun, 26 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>https://ntoofu.github.io/blog/post/eclipse-titan/</guid>
      <description>TTCN-3について気になったので試してみた。
TTCN-3 について TTCN-3 とは 通信関係のconformance testing (仕様や標準などに準拠しているかを調べるテストのこと) 用の言語のひとつ テレコム系とかで使われていたりする？（推測） プロトコル通りに動作するかを確かめたりするためのものなので, 通信のやり取りを柔軟に記述できる 対象とするレイヤ次第ではIPパケットやEthernetフレームのレベルで操作でき, 自由にヘッダを設定して送信したりできる 指定したパターンを満たすメッセージ(フレームやパケット等)を受け取ったら…という条件分岐も簡単に書ける TTCN-3 の実行モデル parallelなテスト実行をサポートしていて, 1つのテストで複数のホスト間のやり取りを記述することが出来る componentと呼ばれるある種のオブジェクトとそのcomponent間のやりとりをテストに記述でき, componentをそれぞれ別のホストに割り当てて動作させることが出来る テスト用の実行ファイルを各テスト実行用ホスト(host controller)に配置して実行すると, 制御用ホスト(main controller)にTCP接続し, 制御用ホスト上に存在する設定ファイルに基づいてテストが実行され, テストの結果やログは制御用ホストに収集される TTCN-3 でできそうなこと ネットワーク機器やNFVなどが期待通りの動作をしているかのテスト 例えばファイアウォールを挟んだ2ホスト間で通信を行い, 期待通りにパケットを通過・遮断させるのか あるプロトコルを実装したソフトウェアが, そのプロトコルでの特定のリクエスト等に対して期待した応答をするか 例えばDNSサーバーに適当なクエリを投げ, 期待した応答が得られるか こちらの書籍の中では例としてそのような題材を扱っていた Eclipse Titanについて TTCN-3自体は標準化された言語であり, その処理系の1つとしてEclipse Titanが存在する その名の通りEclipse Foundation傘下のプロジェクト TTCN-3のファイルをEclipse Titanでコンパイル(トランスパイル?)するとC++のコードになり, これをコンパイルしてテスト実行用のバイナリを得る…といった流れになる TTCN-3の書き方も含めて, 使い方については https://projects.eclipse.org/projects/tools.titan/downloads にまとめて公開されているPDFなどを参照するのが良い。 試してみた https://github.com/ntoofu/ttcn3-titan-experiments に試したものを置いた。以下で簡単に解説しておく。
2つのテストを書いているが, いずれも docker-compose up すれば実行できるように書いている。
01-tcp_syn_response graph TD subgraph mc MTC end subgraph sut nc end MTC -- SYN --&gt; nc nc -.</description>
    </item>
    
    <item>
      <title>VMware ESXi 仮想スイッチのreverse teamingについて</title>
      <link>https://ntoofu.github.io/blog/post/vswitch_reverse_teaming/</link>
      <pubDate>Mon, 04 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://ntoofu.github.io/blog/post/vswitch_reverse_teaming/</guid>
      <description>VMware ESXi の仮想スイッチの挙動に関して同僚に質問され調べたことがあったので、軽くまとめておく。
TL;DR VMware ESXi の仮想スイッチではreverse teamingと呼ばれるフィルタが機能することがある たとえpromiscuous modeを有効にしていても機能するため、ミラーポート代わりにしたい時は注意が必要 前提 注意 この記事の内容はESXi 6.7に対して確認しており、それ以外については確認していない。
NICチーミングについて 詳細はドキュメントにあるので、概要についてのみ記述する。
仮想マシンがESXi外部と通信するために、ESXiの仮想スイッチに対してESXiホストが持つNICを割り当てる必要がある。（仮想スイッチのアップリンクと呼ばれる。） 冗長化や負荷分散のために複数のNICをこのアップリンクとして指定することができ、これをNICチーミングという。
イメージとしてはリンクアグリゲーションが近いし、実際に対向となるスイッチでLAGの設定をしなければならない構成もあるが、LAGを設定しないでも、NICチーミングを利用できる方法も用意されている。 チーミングでは複数NICにどのように通信を分散させるかを規定したロードバランシングポリシーを選択でき、送信元の仮想マシン（正確には仮想スイッチのポート）ごとに利用する物理NICを決定したり、送信元MACアドレスから決定する方法などを選択した場合は、対向スイッチでLAGを設定する必要がない。 （むしろ、設定してはいけない。）
reverse teaming 以前トラブルシューティング時に存在に気づいたが、あまり言及されるのをみないreverse teamingと呼ばれるらしい機能が仮想スイッチには存在している。これはどうやら前述のNICチーミングにおいてLAGを必要としないケースで動作するもののようだ。
スイッチにLAGが設定されなければ、BUMトラフィックについてはフラッディングするため、仮想スイッチの複数のアップリンクに複製された同一のフレームが到達することとなる。したがって、宛先MACアドレスから転送先の仮想マシンを単純に決定するだけでは、複製されたフレームを仮想マシンが受け取ることとなる。それを防ぐため、上述のロードバランシングポリシーが選択されている場合は、仮想マシンがフレーム送信時に利用しているアップリンクで受け取ったパケットしか仮想マシンに転送しないようフィルタリングがなされる。これがreverse teamingと呼ばれるものである。
送信時に利用しているアップリンクの対向ポートにて、仮想マシンのMACアドレスが学習されているはずなので、仮想マシン宛のユニキャスト通信は送信時に利用しているアップリンクで受け取ることになるという想定で、そのような作りになっているのだと思われる。
この機能自体はよく考えられているが、細かい部分でトラブルシューティング時などに影響してくるので、気をつける必要もある。
ESXiでのパケットキャプチャ時の注意 ESXiでは pktcap-uw コマンドによりパケットキャプチャが可能で、アップリンクや仮想マシンが接続される仮想スイッチのポートなどを指定してキャプチャを取得できる。 この時、キャプチャポイントを --capture オプションにより指定できるが、この指定によってreverse teamingが作用した後の結果が取れるかそうでないかが変わってくるようだ。
具体的には、 --capture PortOutput では、reverse teamingが働く前の結果が取得される。 仮想スイッチのポートを指定してキャプチャした際、特にキャプチャポイントを --capture で指定しなかった場合は、デフォルトでこの状態になると思われる。 明示的に --capture VnicRx を指定した場合は、reverse teamingを反映した結果となる。
promiscuous mode と reverse teaming promiscuous mode 通常仮想スイッチはMACアドレスの学習はしないものの、仮想マシンに割り当てているMACアドレスを元に仮想マシンを特定し、仮想スイッチが受け取ったパケットの転送を行う。（イメージとしては、MACテーブルに静的にエントリが指定されており、動的な変更がないような状態での動きに近いかもしれない。）そのため、同一ポートグループを利用していたとしても他の仮想マシンへのユニキャスト通信を受け取ることは基本的に無い。
しかし、promiscuous mode(無差別モード)が有効な場合は、そのポートグループ内の全仮想マシンにパケットが転送される。（イメージとしては、ハブの動作に近いかもしれない。）仮想マシンがL2VPNの終端になっている場合や、再仮想化などの都合で、仮想NICに割り当てられているアドレス以外のアドレスを受け取る必要がある場合などに、よく利用される。
ただし、ESXi6.7からは分散仮想スイッチにてMACアドレス学習させるオプションが出来ているので、必要性は薄れているかもしれない。
reverse teaming の影響 検証したところでは、promiscuous modeであるかどうかに関わらず、reverse teamingは同じように動作するらしい。 しかしこの事がかえって複雑な挙動につながることがある。
promiscuous modeでは、そのポートグループ上で仮想マシンがやり取りした通信は、同一ポートグループ上の他の仮想マシンにも見えてしまうことがある。 同一ESXiホスト上であればポートグループ内の全仮想マシンに転送するため、それ自体は自然なことである。</description>
    </item>
    
    <item>
      <title>IPVS (LVS/NAT) とiptables DNAT targetの共存について調べた</title>
      <link>https://ntoofu.github.io/blog/post/research-ipvs-and-dnat/</link>
      <pubDate>Thu, 02 May 2019 00:00:00 +0000</pubDate>
      
      <guid>https://ntoofu.github.io/blog/post/research-ipvs-and-dnat/</guid>
      <description>TL;DR IPVSはデフォルトでconntrack(標準のconnection tracking)を利用しない そのせいでIPVSに処理させるパケットをNATしたりすると期待通り動かないことがある 必要があれば /proc/sys/net/ipv4/vs/conntrack (net.ipv4.vs.conntrack) を設定しよう 背景 iptablesの DNAT targetと IPVS(LVS/NAT構成)を共存させると上手く行かない PREROUTING chainにおいて REDIRECT target によりポート番号をIPVSのvirtual serviceのものに変換するように設定した場合, REDIRECTされるポートに対し接続すると, 戻り通信(SYNに対するSYN+ACK)の送信元ポートがvirtual serviceのものになる（期待されるのは, 変換前のもの) (PREROUTING chainの) REDIRECT targetは内部的には DNAT と同等の動きをするようで, DNAT でも起きる 以下のようにすれば再現する テスト用ホストでnetcatなどで適当にListenしてくれるポートを2つ作り, IPVSでバランシング設定
(while true;do nc -l -p 1001; echo 1001; done)&amp;amp; (while true;do nc -l -p 1002; echo 1002; done)&amp;amp; ipvsadm -A -t 172.16.1.8:1000 -s rr ipvsadm -a -t 172.16.1.8:1000 -r 172.16.1.8:1001 -m ipvsadm -a -t 172.16.1.8:1000 -r 172.</description>
    </item>
    
    <item>
      <title>Looking Glass (KVM)を「例のグラボ」で試した</title>
      <link>https://ntoofu.github.io/blog/post/try-looking-glass-kvm/</link>
      <pubDate>Mon, 11 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>https://ntoofu.github.io/blog/post/try-looking-glass-kvm/</guid>
      <description>Looking Glass というものを「例のグラボ」で試してみた。
Looking Glass とは 同名のものが色々ある気がするが, 今試すのは これ 最近よく耳にするのはホログラフィックディスプレイである Looking Glassだが, これの話ではない ネットワーク関連とかだと指定したホストにICMPパケットを投げてくれるサービスをLooking Glassと 言うが, それの話でもない KVM (Kernel-based Virtual Machine) にグラフィックスカードをPass-throughで見せているときに, そのグラフィックスカードの出力をKVMホスト上で覗き見るソフトウェア 低遅延であることが大きな特長 KVMによる仮想化環境で, 今の所ゲストがWindows10の場合にのみ利用可能 背景 Linuxデスクトップ上にKVMでWindows10をゲスト動作させ, そこでDirectXを利用するようなゲームを行う&amp;hellip; といったユースケースが主に想定される LinuxでWindowsゲームを行う場合, 以下のような選択肢が主に考えられる (Looking Glassはこの2番めの方法) Proton Windows APIを変換することでWindowsアプリケーションを Linux等のOSで動かそうとする Wine をforkしている Vulkan APIというクロスプラットフォームな低レイヤーのグラフィックスAPIを利用して, DirectXを実装(DXVK)することで, Linux上でDirectXを処理できるようにしている Linux上で仮想マシンを動作させ, WindowsをゲストOSとして利用する DirectXの処理をさせるため, ホスト上のグラフィックスカードをパススルーでゲストOSに見せる WindowsゲームがDirectXではなくVulkanを用いて作られるようになるのを待つ Protonについては試していないのでなんともわからないが, 仮想マシンにグラフィックスカードを パススルーする場合, そのグラフィックスカードの画面出力を利用することになる つまり, KVMホストのLinuxでも画面出力を利用する場合は, Linux用とゲストのWindows用に 2つのディスプレイ（又は1つのディスプレイの2つの画面入力）を消費しないといけない この点を遅延をほとんど生まずに解消できるのがLooking Glassのメリット 但し, 後述の通り実際には画面の接続だけはしたほうがよかったりする 動作原理 パススルーしたグラフィックスカードのフレームバッファ（各種レンダリングの結果とも言える, 実際に画面へ出力するデータを格納している）の中身を, KVMホストから参照してKVMホスト側の GUI環境に描画する DXGI Desktop Duplicationという, 画面キャプチャに用いるDirectX関連のAPIを使って, フレームをメインメモリ上にコピーするプログラムをゲストWindows上で動作させる ゲストOSにてメインメモリ上に取ってきたフレームは, ivshmem (Inter-VM shared memory) というKVMの機能を使って, KVMホストから参照できるようにする KVMホストのGUI環境でクライアントのソフトウェアを動作させ, ivshmemを参照してフレームを描画する graph LR subgraph KVM_host subgraph guest_Windows agent((&#34;</description>
    </item>
    
    <item>
      <title>Multus CNI pluginをKubernetesで試した</title>
      <link>https://ntoofu.github.io/blog/post/try-multus-cni/</link>
      <pubDate>Sun, 10 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>https://ntoofu.github.io/blog/post/try-multus-cni/</guid>
      <description>先日の記事でも軽くふれた Multus CNI plugin を実際に使ってみた。
（盛んに開発されているため, 執筆時から内容が変わっている可能性があるので注意）
Multus CNI について CNI plugin を複数呼び出すことで, container に複数のNetwork interfaceを与えるCNI plugin Kubernetes と連携し, Pod の Manifest に応じて利用するNetworkを切り替えたりも可能 Kubernetes連携時の動作の仕組み 各ノードのCNI設定ファイル(デフォルト: /etc/cni/net.d/)を設定し, Multusが呼び出されるようにする 現時点ではDaemonSetとして作成したPodを使ってノードに対するCNI設定ファイル配置を実現している nfvpe/multus というコンテナイメージに, 設定ファイルを配置するべきホストのディレクトリを volumeとしてマウントさせる 設定内容はコンテナ内のファイルにあるので必要に応じてdaemon用コンテナにConfigMapを volumeマウントさせる 半年ほど前に触ったときは, ホスト側の設定(/etc/cni/net.d/)を直接編集する手順が案内されていた (と思う)ので, また変わるかもしれない 軽く見た限り, Calicoやflannelも同じようなことをしていそう 各ノードにMultusを含む利用したいCNI pluginバイナリを配置しておき, KubeletがMultusを呼び出し, Multusが1つ以上のCNI pluginを呼び出す Multus CNI pluginのバイナリ配置については前述の DaemonSet がやってくれる Podには必ず1つ共通のネットワーク (=&amp;ldquo;Default network&amp;rdquo;) が eth0 として設定される おそらくKubernetesのPod Networkとしての要件を満たすため Custom Resource Definition (CRD) という Kubenetes が提供しているAPI拡張の仕組みを利用し, Default network以外のネットワークを定義し, PodはManifestにてこれを指定する Multus CNI pluginはそれ自体がKubernetes APIを呼び出し, CRDを参照してネットワークに対して どのCNI pluginをどういうconfigで呼び出すかを決定する セットアップ ここでは, kubeadm のインストールが完了し, Kubenetesクラスタ構築直前の状態からの構築を想定する。 (Creating a single master cluster with kubeadmの, &amp;ldquo;Installing a pod network add-on&amp;rdquo; の直前の状態) 今回, Default networkにはProject Calicoを利用してみる。</description>
    </item>
    
    <item>
      <title>Container Network Interfaceについて調べた</title>
      <link>https://ntoofu.github.io/blog/post/container-network-interface/</link>
      <pubDate>Wed, 16 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://ntoofu.github.io/blog/post/container-network-interface/</guid>
      <description>社内のKubernetes勉強会で, 前より関心のあった Container Network Interface (CNI) の説明を することになった。その際の資料をここに書いておく。
Container Network Interface (CNI) の基本 CNIとは何か Linux container内のnetwork interfaceを設定するプラグインを書くための 仕様とライブラリからなるCNCFプロジェクト containerのネットワーク接続性を作ることと削除時の後始末だけを考えているため, 仕様は単純でサポート状況も良い(k8s専用というわけでもない) GitHub CNIが生まれてきた背景 Linux containerが急速に発展しcontainer runtimeやオーケストレータが複数出てきた 環境依存しやすいネットワーク周りをplugableにする方法は確立されていなかった それぞれが独自に問題を解決する無駄を避けるべく, CNIが共通のインターフェイスを定める 元はと言えば rkt 用に考えられたものらしい CNI plugins CNIに準拠したpluginはたくさんあり, READMEにも書いてある Golangで書かれたものは多いが, CNIは言語については規定していないため何で書いても良い CNI仕様 SPEC.mdに仕様が記されている CNI からすれば container = Linux network namespace (netns) (runtimeによりnetnsを作る単位が異なるが, CNIとしてはcontainer = network namespaceとみなす) そもそも, Linux containerから見えるネットワークインターフェイスは コンテナ用のnetns内のinterfaceでしかない &amp;ldquo;network&amp;rdquo; ごとに, どのpluginを用いるかなどの設定をCNIが定めるJSON形式で用意する name: host内で一意な, networkを表す名前 type: そのnetworkで使うCNI pluginの実行ファイル名を指定 cniVersion: CNI specのバージョン ipam: I/FへのIPアドレス割り当てを管理するIPAM pluginの設定 dns: container(netns)に対するDNSを設定する その他, plugin固有の設定 (e.</description>
    </item>
    
    <item>
      <title>git-annex によるファイル管理を試した</title>
      <link>https://ntoofu.github.io/blog/post/git-annex/</link>
      <pubDate>Mon, 07 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://ntoofu.github.io/blog/post/git-annex/</guid>
      <description>LWN.netの記事 で git-annex について知ったのでちょっと試してみた。
git-annex LWN.netの記事もそうであるように, Gitで巨大なファイルを管理する際に git-lfsと並んで名前があがることがある 大きなファイルに限らず, ファイルの実体とバージョン管理を分離することができる ユースケースについては git-annex の説明を 読むのがわかり易い 例1: バックアップ・アーカイブ用途 例2: キャッシュ的な用途（クラウドストレージにあるデータをリムーバブルメディアに持ってきて 利用した後, 書き戻す） 柔軟な制御は出来る反面面倒な部分が多い 実体の置き場を複数に分散させられるので, 分散ストレージもどきっぽくできる 置き場にはクラウドストレージ等も利用可能 保持しておくべき実体のコピーの数とかも指定できる（バックアップ用途に便利） 実体が手元にあるかないか気にする必要があるが, 裏を返せば実体は必要なものだけ持つこともできる バージョン管理はgitの流儀に従って都度自分で行うか自動でやる仕組みを整える必要がある cronジョブを仕込むとか ホスティングサービスの対応は薄い GitLabでは対応を打ち切られている ちなみに haskell で書かれている模様 検証のモチベーション デスクトップ, ラップトップ, モバイルのデータ共有に使えないか 以下のような運用を実現したい デスクトップは常にアクセスできるNFS上のデータにアクセスするようにしておきたい NFS上のデータは適宜クラウドストレージ(Google Drive)にバックアップとしてデータをコピーしておきたい ラップトップは自宅ではNFSのデータを使いつつ, 外出先ではローカルのデータを使い, もしローカルに無い場合はクラウドストレージからデータを持ってくるようにする ローカルの容量が小さいので, 必要なデータだけを持ってきたい モバイル(iOS)から参照が必要なものは都度クラウドストレージから読み出しつつ, 撮った写真とかはクラウドストレージにアップロードする gdriveなどでrsyncライクに同期を取ることはやっていたが, 気になる部分があってやめていた 同期忘れで並行して変更が発生してしまい不整合が出たら嫌 NFSを使っている関係で書き換え発生時に自動的に同期することが出来ない 試してわかったこと 公式にwalkthroughがあるので, 参考にして色々試しつつ動きを確認した。
git-annex version: 7.20181211 インストール Gentooでは haskell リポジトリにある (余談) xmonad も同様のため自分の環境ではすんなり入れられた annexを使う準備として, 対象になるGitのworking tree内で git annex init .</description>
    </item>
    
    <item>
      <title>GitHub Pagesの利用を再開した</title>
      <link>https://ntoofu.github.io/blog/post/github-pages/</link>
      <pubDate>Sun, 06 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://ntoofu.github.io/blog/post/github-pages/</guid>
      <description>元々GitHub Pagesを使ってコンピュータ関連の技術系のブログ（所謂技術ブログ）を公開しようと 1年ぐらい前にやっていて、実際Jekyllで書けるようにするところまではやっていた。 が、特に何も書かずに放置してしまっていたので、整備し直すついでにHugoに乗り換えた。 （markdownのドキュメントをHugoでHTMLにして見るようにするということを業務でやっていて、 両方を触るのが面倒だったため）
大事なのは記事をちゃんと書くことで、今までは何か書くことが出来たらーという気持ちで 構えていたのが良くなかったなと思う。これ以上やることはないというぐらいに やり尽くしたり学び尽くすなどという事は大体の事柄に対してあり得ないので、 やりきってから書くつもりだと中々その時が訪れない。 最低月1つは何か書くことにする。
また、調べたことをまとめて記事にすることも当初は考えていたが、いざとなると 「自分もネットで調べて知識を得ているのだから、まとめて書いたところで新規性も無いただの劣化コピーになるだけ」 と思ってしまい書く気にならなかった。 しかし、GitHub Pagesで書いてサクッと公開するぐらいならSEO的にもそんなに人目に触れないと思うので、 本当に私的なメモを（見ようと思えば）誰にでも見れる場所に置いているだけ、という気持ちで取り組もうと思う。
しょうもない事を長々書いたが、基本的には何かしら技術に関する話題を含む記事を今後定期的に書いていきたいと思う。</description>
    </item>
    
  </channel>
</rss>
