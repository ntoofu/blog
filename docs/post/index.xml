<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on ntoofu</title>
    <link>https://ntoofu.github.io/blog/post/</link>
    <description>Recent content in Posts on ntoofu</description>
    <generator>Hugo -- gohugo.io</generator>
    <copyright>&amp;copy; 2018 ntoofu</copyright>
    <lastBuildDate>Fri, 04 Jan 2019 00:17:21 +0900</lastBuildDate>
    
	<atom:link href="https://ntoofu.github.io/blog/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Looking Glass (KVM)を「例のグラボ」で試した</title>
      <link>https://ntoofu.github.io/blog/post/try-looking-glass-kvm/</link>
      <pubDate>Mon, 11 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>https://ntoofu.github.io/blog/post/try-looking-glass-kvm/</guid>
      <description>Looking Glass というものを「例のグラボ」で試してみた。
Looking Glass とは  同名のものが色々ある気がするが, 今試すのは これ  最近よく耳にするのはホログラフィックディスプレイである Looking Glassだが, これの話ではない ネットワーク関連とかだと指定したホストにICMPパケットを投げてくれるサービスをLooking Glassと 言うが, それの話でもない  KVM (Kernel-based Virtual Machine) にグラフィックスカードをPass-throughで見せているときに, そのグラフィックスカードの出力をKVMホスト上で覗き見るソフトウェア  低遅延であることが大きな特長  KVMによる仮想化環境で, 今の所ゲストがWindows10の場合にのみ利用可能  背景  Linuxデスクトップ上にKVMでWindows10をゲスト動作させ, そこでDirectXを利用するようなゲームを行う&amp;hellip; といったユースケースが主に想定される LinuxでWindowsゲームを行う場合, 以下のような選択肢が主に考えられる (Looking Glassはこの2番めの方法)  Proton  Windows APIを変換することでWindowsアプリケーションを Linux等のOSで動かそうとする Wine をforkしている Vulkan APIというクロスプラットフォームな低レイヤーのグラフィックスAPIを利用して, DirectXを実装(DXVK)することで, Linux上でDirectXを処理できるようにしている  Linux上で仮想マシンを動作させ, WindowsをゲストOSとして利用する  DirectXの処理をさせるため, ホスト上のグラフィックスカードをパススルーでゲストOSに見せる  WindowsゲームがDirectXではなくVulkanを用いて作られるようになるのを待つ  Protonについては試していないのでなんともわからないが, 仮想マシンにグラフィックスカードを パススルーする場合, そのグラフィックスカードの画面出力を利用することになる  つまり, KVMホストのLinuxでも画面出力を利用する場合は, Linux用とゲストのWindows用に 2つのディスプレイ（又は1つのディスプレイの2つの画面入力）を消費しないといけない この点を遅延をほとんど生まずに解消できるのがLooking Glassのメリット  但し, 後述の通り実際には画面の接続だけはしたほうがよかったりする    動作原理  パススルーしたグラフィックスカードのフレームバッファ（各種レンダリングの結果とも言える, 実際に画面へ出力するデータを格納している）の中身を, KVMホストから参照してKVMホスト側の GUI環境に描画する  DXGI Desktop Duplicationという, 画面キャプチャに用いるDirectX関連のAPIを使って, フレームをメインメモリ上にコピーするプログラムをゲストWindows上で動作させる ゲストOSにてメインメモリ上に取ってきたフレームは, ivshmem (Inter-VM shared memory) というKVMの機能を使って, KVMホストから参照できるようにする KVMホストのGUI環境でクライアントのソフトウェアを動作させ, ivshmemを参照してフレームを描画する   mermaid.</description>
    </item>
    
    <item>
      <title>Multus CNI pluginをKubernetesで試した</title>
      <link>https://ntoofu.github.io/blog/post/try-multus-cni/</link>
      <pubDate>Sun, 10 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>https://ntoofu.github.io/blog/post/try-multus-cni/</guid>
      <description>先日の記事でも軽くふれた Multus CNI plugin を実際に使ってみた。
（盛んに開発されているため, 執筆時から内容が変わっている可能性があるので注意）
Multus CNI について  CNI plugin を複数呼び出すことで, container に複数のNetwork interfaceを与えるCNI plugin Kubernetes と連携し, Pod の Manifest に応じて利用するNetworkを切り替えたりも可能  Kubernetes連携時の動作の仕組み  各ノードのCNI設定ファイル(デフォルト: /etc/cni/net.d/)を設定し, Multusが呼び出されるようにする  現時点ではDaemonSetとして作成したPodを使ってノードに対するCNI設定ファイル配置を実現している  nfvpe/multus というコンテナイメージに, 設定ファイルを配置するべきホストのディレクトリを volumeとしてマウントさせる 設定内容はコンテナ内のファイルにあるので必要に応じてdaemon用コンテナにConfigMapを volumeマウントさせる  半年ほど前に触ったときは, ホスト側の設定(/etc/cni/net.d/)を直接編集する手順が案内されていた (と思う)ので, また変わるかもしれない  軽く見た限り, Calicoやflannelも同じようなことをしていそう   各ノードにMultusを含む利用したいCNI pluginバイナリを配置しておき, KubeletがMultusを呼び出し, Multusが1つ以上のCNI pluginを呼び出す  Multus CNI pluginのバイナリ配置については前述の DaemonSet がやってくれる  Podには必ず1つ共通のネットワーク (=&amp;ldquo;Default network&amp;rdquo;) が eth0 として設定される  おそらくKubernetesのPod Networkとしての要件を満たすため  Custom Resource Definition (CRD) という Kubenetes が提供しているAPI拡張の仕組みを利用し, Default network以外のネットワークを定義し, PodはManifestにてこれを指定する Multus CNI pluginはそれ自体がKubernetes APIを呼び出し, CRDを参照してネットワークに対して どのCNI pluginをどういうconfigで呼び出すかを決定する  セットアップ ここでは, kubeadm のインストールが完了し, Kubenetesクラスタ構築直前の状態からの構築を想定する。 (Creating a single master cluster with kubeadmの, &amp;ldquo;Installing a pod network add-on&amp;rdquo; の直前の状態) 今回, Default networkにはProject Calicoを利用してみる。</description>
    </item>
    
    <item>
      <title>Container Network Interfaceについて調べた</title>
      <link>https://ntoofu.github.io/blog/post/container-network-interface/</link>
      <pubDate>Wed, 16 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://ntoofu.github.io/blog/post/container-network-interface/</guid>
      <description>社内のKubernetes勉強会で, 前より関心のあった Container Network Interface (CNI) の説明を することになった。その際の資料をここに書いておく。
Container Network Interface (CNI) の基本 CNIとは何か  Linux container内のnetwork interfaceを設定するプラグインを書くための 仕様とライブラリからなるCNCFプロジェクト containerのネットワーク接続性を作ることと削除時の後始末だけを考えているため, 仕様は単純でサポート状況も良い(k8s専用というわけでもない) GitHub  CNIが生まれてきた背景  Linux containerが急速に発展しcontainer runtimeやオーケストレータが複数出てきた 環境依存しやすいネットワーク周りをplugableにする方法は確立されていなかった それぞれが独自に問題を解決する無駄を避けるべく, CNIが共通のインターフェイスを定める  元はと言えば rkt 用に考えられたものらしい   CNI plugins  CNIに準拠したpluginはたくさんあり, READMEにも書いてある Golangで書かれたものは多いが, CNIは言語については規定していないため何で書いても良い  CNI仕様  SPEC.mdに仕様が記されている CNI からすれば container = Linux network namespace (netns) (runtimeによりnetnsを作る単位が異なるが, CNIとしてはcontainer = network namespaceとみなす)  そもそも, Linux containerから見えるネットワークインターフェイスは コンテナ用のnetns内のinterfaceでしかない  &amp;ldquo;network&amp;rdquo; ごとに, どのpluginを用いるかなどの設定をCNIが定めるJSON形式で用意する  name: host内で一意な, networkを表す名前 type: そのnetworkで使うCNI pluginの実行ファイル名を指定 cniVersion: CNI specのバージョン ipam: I/FへのIPアドレス割り当てを管理するIPAM pluginの設定 dns: container(netns)に対するDNSを設定する その他, plugin固有の設定 (e.</description>
    </item>
    
    <item>
      <title>git-annex によるファイル管理を試した</title>
      <link>https://ntoofu.github.io/blog/post/git-annex/</link>
      <pubDate>Mon, 07 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://ntoofu.github.io/blog/post/git-annex/</guid>
      <description>LWN.netの記事 で git-annex について知ったのでちょっと試してみた。
git-annex  LWN.netの記事もそうであるように, Gitで巨大なファイルを管理する際に git-lfsと並んで名前があがることがある 大きなファイルに限らず, ファイルの実体とバージョン管理を分離することができる ユースケースについては git-annex の説明を 読むのがわかり易い  例1: バックアップ・アーカイブ用途 例2: キャッシュ的な用途（クラウドストレージにあるデータをリムーバブルメディアに持ってきて 利用した後, 書き戻す）  柔軟な制御は出来る反面面倒な部分が多い  実体の置き場を複数に分散させられるので, 分散ストレージもどきっぽくできる  置き場にはクラウドストレージ等も利用可能  保持しておくべき実体のコピーの数とかも指定できる（バックアップ用途に便利） 実体が手元にあるかないか気にする必要があるが, 裏を返せば実体は必要なものだけ持つこともできる バージョン管理はgitの流儀に従って都度自分で行うか自動でやる仕組みを整える必要がある  cronジョブを仕込むとか   ホスティングサービスの対応は薄い  GitLabでは対応を打ち切られている  ちなみに haskell で書かれている模様  検証のモチベーション  デスクトップ, ラップトップ, モバイルのデータ共有に使えないか 以下のような運用を実現したい  デスクトップは常にアクセスできるNFS上のデータにアクセスするようにしておきたい NFS上のデータは適宜クラウドストレージ(Google Drive)にバックアップとしてデータをコピーしておきたい ラップトップは自宅ではNFSのデータを使いつつ, 外出先ではローカルのデータを使い, もしローカルに無い場合はクラウドストレージからデータを持ってくるようにする  ローカルの容量が小さいので, 必要なデータだけを持ってきたい  モバイル(iOS)から参照が必要なものは都度クラウドストレージから読み出しつつ, 撮った写真とかはクラウドストレージにアップロードする  gdriveなどでrsyncライクに同期を取ることはやっていたが, 気になる部分があってやめていた  同期忘れで並行して変更が発生してしまい不整合が出たら嫌 NFSを使っている関係で書き換え発生時に自動的に同期することが出来ない   試してわかったこと 公式にwalkthroughがあるので, 参考にして色々試しつつ動きを確認した。</description>
    </item>
    
    <item>
      <title>GitHub Pagesの利用を再開した</title>
      <link>https://ntoofu.github.io/blog/post/github-pages/</link>
      <pubDate>Sun, 06 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://ntoofu.github.io/blog/post/github-pages/</guid>
      <description>元々GitHub Pagesを使ってコンピュータ関連の技術系のブログ（所謂技術ブログ）を公開しようと 1年ぐらい前にやっていて、実際Jekyllで書けるようにするところまではやっていた。 が、特に何も書かずに放置してしまっていたので、整備し直すついでにHugoに乗り換えた。 （markdownのドキュメントをHugoでHTMLにして見るようにするということを業務でやっていて、 両方を触るのが面倒だったため）
大事なのは記事をちゃんと書くことで、今までは何か書くことが出来たらーという気持ちで 構えていたのが良くなかったなと思う。これ以上やることはないというぐらいに やり尽くしたり学び尽くすなどという事は大体の事柄に対してあり得ないので、 やりきってから書くつもりだと中々その時が訪れない。 最低月1つは何か書くことにする。
また、調べたことをまとめて記事にすることも当初は考えていたが、いざとなると 「自分もネットで調べて知識を得ているのだから、まとめて書いたところで新規性も無いただの劣化コピーになるだけ」 と思ってしまい書く気にならなかった。 しかし、GitHub Pagesで書いてサクッと公開するぐらいならSEO的にもそんなに人目に触れないと思うので、 本当に私的なメモを（見ようと思えば）誰にでも見れる場所に置いているだけ、という気持ちで取り組もうと思う。
しょうもない事を長々書いたが、基本的には何かしら技術に関する話題を含む記事を今後定期的に書いていきたいと思う。</description>
    </item>
    
  </channel>
</rss>