<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on ntoofu</title>
    <link>https://ntoofu.github.io/blog/post/</link>
    <description>Recent content in Posts on ntoofu</description>
    <generator>Hugo -- gohugo.io</generator>
    <copyright>&amp;copy; 2018 ntoofu</copyright>
    <lastBuildDate>Fri, 04 Jan 2019 00:17:21 +0900</lastBuildDate>
    
	<atom:link href="https://ntoofu.github.io/blog/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Eclipse TitanでTTCN-3を試した</title>
      <link>https://ntoofu.github.io/blog/post/eclipse-titan/</link>
      <pubDate>Sun, 26 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>https://ntoofu.github.io/blog/post/eclipse-titan/</guid>
      <description>TTCN-3について気になったので試してみた。
TTCN-3 について TTCN-3 とは  通信関係のconformance testing (仕様や標準などに準拠しているかを調べるテストのこと) 用の言語のひとつ  テレコム系とかで使われていたりする？（推測）  プロトコル通りに動作するかを確かめたりするためのものなので, 通信のやり取りを柔軟に記述できる  対象とするレイヤ次第ではIPパケットやEthernetフレームのレベルで操作でき, 自由にヘッダを設定して送信したりできる 指定したパターンを満たすメッセージ(フレームやパケット等)を受け取ったら…という条件分岐も簡単に書ける   TTCN-3 の実行モデル  parallelなテスト実行をサポートしていて, 1つのテストで複数のホスト間のやり取りを記述することが出来る  componentと呼ばれるある種のオブジェクトとそのcomponent間のやりとりをテストに記述でき, componentをそれぞれ別のホストに割り当てて動作させることが出来る テスト用の実行ファイルを各テスト実行用ホスト(host controller)に配置して実行すると, 制御用ホスト(main controller)にTCP接続し, 制御用ホスト上に存在する設定ファイルに基づいてテストが実行され, テストの結果やログは制御用ホストに収集される   TTCN-3 でできそうなこと  ネットワーク機器やNFVなどが期待通りの動作をしているかのテスト  例えばファイアウォールを挟んだ2ホスト間で通信を行い, 期待通りにパケットを通過・遮断させるのか  あるプロトコルを実装したソフトウェアが, そのプロトコルでの特定のリクエスト等に対して期待した応答をするか  例えばDNSサーバーに適当なクエリを投げ, 期待した応答が得られるか  こちらの書籍の中では例としてそのような題材を扱っていた    Eclipse Titanについて  TTCN-3自体は標準化された言語であり, その処理系の1つとしてEclipse Titanが存在する その名の通りEclipse Foundation傘下のプロジェクト TTCN-3のファイルをEclipse Titanでコンパイル(トランスパイル?)するとC++のコードになり, これをコンパイルしてテスト実行用のバイナリを得る…といった流れになる TTCN-3の書き方も含めて, 使い方については https://projects.eclipse.org/projects/tools.titan/downloads にまとめて公開されているPDFなどを参照するのが良い。  試してみた https://github.</description>
    </item>
    
    <item>
      <title>VMware ESXi 仮想スイッチのreverse teamingについて</title>
      <link>https://ntoofu.github.io/blog/post/vswitch_reverse_teaming/</link>
      <pubDate>Mon, 04 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://ntoofu.github.io/blog/post/vswitch_reverse_teaming/</guid>
      <description>VMware ESXi の仮想スイッチの挙動に関して同僚に質問され調べたことがあったので、軽くまとめておく。
TL;DR  VMware ESXi の仮想スイッチではreverse teamingと呼ばれるフィルタが機能することがある たとえpromiscuous modeを有効にしていても機能するため、ミラーポート代わりにしたい時は注意が必要  前提 注意 この記事の内容はESXi 6.7に対して確認しており、それ以外については確認していない。
NICチーミングについて 詳細はドキュメントにあるので、概要についてのみ記述する。
仮想マシンがESXi外部と通信するために、ESXiの仮想スイッチに対してESXiホストが持つNICを割り当てる必要がある。（仮想スイッチのアップリンクと呼ばれる。） 冗長化や負荷分散のために複数のNICをこのアップリンクとして指定することができ、これをNICチーミングという。
イメージとしてはリンクアグリゲーションが近いし、実際に対向となるスイッチでLAGの設定をしなければならない構成もあるが、LAGを設定しないでも、NICチーミングを利用できる方法も用意されている。 チーミングでは複数NICにどのように通信を分散させるかを規定したロードバランシングポリシーを選択でき、送信元の仮想マシン（正確には仮想スイッチのポート）ごとに利用する物理NICを決定したり、送信元MACアドレスから決定する方法などを選択した場合は、対向スイッチでLAGを設定する必要がない。 （むしろ、設定してはいけない。）
reverse teaming 以前トラブルシューティング時に存在に気づいたが、あまり言及されるのをみないreverse teamingと呼ばれるらしい機能が仮想スイッチには存在している。これはどうやら前述のNICチーミングにおいてLAGを必要としないケースで動作するもののようだ。
スイッチにLAGが設定されなければ、BUMトラフィックについてはフラッディングするため、仮想スイッチの複数のアップリンクに複製された同一のフレームが到達することとなる。したがって、宛先MACアドレスから転送先の仮想マシンを単純に決定するだけでは、複製されたフレームを仮想マシンが受け取ることとなる。それを防ぐため、上述のロードバランシングポリシーが選択されている場合は、仮想マシンがフレーム送信時に利用しているアップリンクで受け取ったパケットしか仮想マシンに転送しないようフィルタリングがなされる。これがreverse teamingと呼ばれるものである。
送信時に利用しているアップリンクの対向ポートにて、仮想マシンのMACアドレスが学習されているはずなので、仮想マシン宛のユニキャスト通信は送信時に利用しているアップリンクで受け取ることになるという想定で、そのような作りになっているのだと思われる。
この機能自体はよく考えられているが、細かい部分でトラブルシューティング時などに影響してくるので、気をつける必要もある。
ESXiでのパケットキャプチャ時の注意 ESXiでは pktcap-uw コマンドによりパケットキャプチャが可能で、アップリンクや仮想マシンが接続される仮想スイッチのポートなどを指定してキャプチャを取得できる。 この時、キャプチャポイントを --capture オプションにより指定できるが、この指定によってreverse teamingが作用した後の結果が取れるかそうでないかが変わってくるようだ。
具体的には、 --capture PortOutput では、reverse teamingが働く前の結果が取得される。 仮想スイッチのポートを指定してキャプチャした際、特にキャプチャポイントを --capture で指定しなかった場合は、デフォルトでこの状態になると思われる。 明示的に --capture VnicRx を指定した場合は、reverse teamingを反映した結果となる。
promiscuous mode と reverse teaming promiscuous mode 通常仮想スイッチはMACアドレスの学習はしないものの、仮想マシンに割り当てているMACアドレスを元に仮想マシンを特定し、仮想スイッチが受け取ったパケットの転送を行う。（イメージとしては、MACテーブルに静的にエントリが指定されており、動的な変更がないような状態での動きに近いかもしれない。）そのため、同一ポートグループを利用していたとしても他の仮想マシンへのユニキャスト通信を受け取ることは基本的に無い。
しかし、promiscuous mode(無差別モード)が有効な場合は、そのポートグループ内の全仮想マシンにパケットが転送される。（イメージとしては、ハブの動作に近いかもしれない。）仮想マシンがL2VPNの終端になっている場合や、再仮想化などの都合で、仮想NICに割り当てられているアドレス以外のアドレスを受け取る必要がある場合などに、よく利用される。
ただし、ESXi6.7からは分散仮想スイッチにてMACアドレス学習させるオプションが出来ているので、必要性は薄れているかもしれない。
reverse teaming の影響 検証したところでは、promiscuous modeであるかどうかに関わらず、reverse teamingは同じように動作するらしい。 しかしこの事がかえって複雑な挙動につながることがある。</description>
    </item>
    
    <item>
      <title>IPVS (LVS/NAT) とiptables DNAT targetの共存について調べた</title>
      <link>https://ntoofu.github.io/blog/post/research-ipvs-and-dnat/</link>
      <pubDate>Thu, 02 May 2019 00:00:00 +0000</pubDate>
      
      <guid>https://ntoofu.github.io/blog/post/research-ipvs-and-dnat/</guid>
      <description>TL;DR  IPVSはデフォルトでconntrack(標準のconnection tracking)を利用しない そのせいでIPVSに処理させるパケットをNATしたりすると期待通り動かないことがある 必要があれば /proc/sys/net/ipv4/vs/conntrack (net.ipv4.vs.conntrack) を設定しよう  背景  iptablesの DNAT targetと IPVS(LVS/NAT構成)を共存させると上手く行かない  PREROUTING chainにおいて REDIRECT target によりポート番号をIPVSのvirtual serviceのものに変換するように設定した場合, REDIRECTされるポートに対し接続すると, 戻り通信(SYNに対するSYN+ACK)の送信元ポートがvirtual serviceのものになる（期待されるのは, 変換前のもの) (PREROUTING chainの) REDIRECT targetは内部的には DNAT と同等の動きをするようで, DNAT でも起きる  以下のようにすれば再現する
 テスト用ホストでnetcatなどで適当にListenしてくれるポートを2つ作り, IPVSでバランシング設定
(while true;do nc -l -p 1001; echo 1001; done)&amp;amp; (while true;do nc -l -p 1002; echo 1002; done)&amp;amp;  ipvsadm -A -t 172.16.1.8:1000 -s rr ipvsadm -a -t 172.16.1.8:1000 -r 172.</description>
    </item>
    
    <item>
      <title>Looking Glass (KVM)を「例のグラボ」で試した</title>
      <link>https://ntoofu.github.io/blog/post/try-looking-glass-kvm/</link>
      <pubDate>Mon, 11 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>https://ntoofu.github.io/blog/post/try-looking-glass-kvm/</guid>
      <description>Looking Glass というものを「例のグラボ」で試してみた。
Looking Glass とは  同名のものが色々ある気がするが, 今試すのは これ  最近よく耳にするのはホログラフィックディスプレイである Looking Glassだが, これの話ではない ネットワーク関連とかだと指定したホストにICMPパケットを投げてくれるサービスをLooking Glassと 言うが, それの話でもない  KVM (Kernel-based Virtual Machine) にグラフィックスカードをPass-throughで見せているときに, そのグラフィックスカードの出力をKVMホスト上で覗き見るソフトウェア  低遅延であることが大きな特長  KVMによる仮想化環境で, 今の所ゲストがWindows10の場合にのみ利用可能  背景  Linuxデスクトップ上にKVMでWindows10をゲスト動作させ, そこでDirectXを利用するようなゲームを行う&amp;hellip; といったユースケースが主に想定される LinuxでWindowsゲームを行う場合, 以下のような選択肢が主に考えられる (Looking Glassはこの2番めの方法)  Proton  Windows APIを変換することでWindowsアプリケーションを Linux等のOSで動かそうとする Wine をforkしている Vulkan APIというクロスプラットフォームな低レイヤーのグラフィックスAPIを利用して, DirectXを実装(DXVK)することで, Linux上でDirectXを処理できるようにしている  Linux上で仮想マシンを動作させ, WindowsをゲストOSとして利用する  DirectXの処理をさせるため, ホスト上のグラフィックスカードをパススルーでゲストOSに見せる  WindowsゲームがDirectXではなくVulkanを用いて作られるようになるのを待つ  Protonについては試していないのでなんともわからないが, 仮想マシンにグラフィックスカードを パススルーする場合, そのグラフィックスカードの画面出力を利用することになる  つまり, KVMホストのLinuxでも画面出力を利用する場合は, Linux用とゲストのWindows用に 2つのディスプレイ（又は1つのディスプレイの2つの画面入力）を消費しないといけない この点を遅延をほとんど生まずに解消できるのがLooking Glassのメリット  但し, 後述の通り実際には画面の接続だけはしたほうがよかったりする    動作原理  パススルーしたグラフィックスカードのフレームバッファ（各種レンダリングの結果とも言える, 実際に画面へ出力するデータを格納している）の中身を, KVMホストから参照してKVMホスト側の GUI環境に描画する  DXGI Desktop Duplicationという, 画面キャプチャに用いるDirectX関連のAPIを使って, フレームをメインメモリ上にコピーするプログラムをゲストWindows上で動作させる ゲストOSにてメインメモリ上に取ってきたフレームは, ivshmem (Inter-VM shared memory) というKVMの機能を使って, KVMホストから参照できるようにする KVMホストのGUI環境でクライアントのソフトウェアを動作させ, ivshmemを参照してフレームを描画する   mermaid.</description>
    </item>
    
    <item>
      <title>Multus CNI pluginをKubernetesで試した</title>
      <link>https://ntoofu.github.io/blog/post/try-multus-cni/</link>
      <pubDate>Sun, 10 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>https://ntoofu.github.io/blog/post/try-multus-cni/</guid>
      <description>先日の記事でも軽くふれた Multus CNI plugin を実際に使ってみた。
（盛んに開発されているため, 執筆時から内容が変わっている可能性があるので注意）
Multus CNI について  CNI plugin を複数呼び出すことで, container に複数のNetwork interfaceを与えるCNI plugin Kubernetes と連携し, Pod の Manifest に応じて利用するNetworkを切り替えたりも可能  Kubernetes連携時の動作の仕組み  各ノードのCNI設定ファイル(デフォルト: /etc/cni/net.d/)を設定し, Multusが呼び出されるようにする  現時点ではDaemonSetとして作成したPodを使ってノードに対するCNI設定ファイル配置を実現している  nfvpe/multus というコンテナイメージに, 設定ファイルを配置するべきホストのディレクトリを volumeとしてマウントさせる 設定内容はコンテナ内のファイルにあるので必要に応じてdaemon用コンテナにConfigMapを volumeマウントさせる  半年ほど前に触ったときは, ホスト側の設定(/etc/cni/net.d/)を直接編集する手順が案内されていた (と思う)ので, また変わるかもしれない  軽く見た限り, Calicoやflannelも同じようなことをしていそう   各ノードにMultusを含む利用したいCNI pluginバイナリを配置しておき, KubeletがMultusを呼び出し, Multusが1つ以上のCNI pluginを呼び出す  Multus CNI pluginのバイナリ配置については前述の DaemonSet がやってくれる  Podには必ず1つ共通のネットワーク (=&amp;ldquo;Default network&amp;rdquo;) が eth0 として設定される  おそらくKubernetesのPod Networkとしての要件を満たすため  Custom Resource Definition (CRD) という Kubenetes が提供しているAPI拡張の仕組みを利用し, Default network以外のネットワークを定義し, PodはManifestにてこれを指定する Multus CNI pluginはそれ自体がKubernetes APIを呼び出し, CRDを参照してネットワークに対して どのCNI pluginをどういうconfigで呼び出すかを決定する  セットアップ ここでは, kubeadm のインストールが完了し, Kubenetesクラスタ構築直前の状態からの構築を想定する。 (Creating a single master cluster with kubeadmの, &amp;ldquo;Installing a pod network add-on&amp;rdquo; の直前の状態) 今回, Default networkにはProject Calicoを利用してみる。</description>
    </item>
    
    <item>
      <title>Container Network Interfaceについて調べた</title>
      <link>https://ntoofu.github.io/blog/post/container-network-interface/</link>
      <pubDate>Wed, 16 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://ntoofu.github.io/blog/post/container-network-interface/</guid>
      <description>社内のKubernetes勉強会で, 前より関心のあった Container Network Interface (CNI) の説明を することになった。その際の資料をここに書いておく。
Container Network Interface (CNI) の基本 CNIとは何か  Linux container内のnetwork interfaceを設定するプラグインを書くための 仕様とライブラリからなるCNCFプロジェクト containerのネットワーク接続性を作ることと削除時の後始末だけを考えているため, 仕様は単純でサポート状況も良い(k8s専用というわけでもない) GitHub  CNIが生まれてきた背景  Linux containerが急速に発展しcontainer runtimeやオーケストレータが複数出てきた 環境依存しやすいネットワーク周りをplugableにする方法は確立されていなかった それぞれが独自に問題を解決する無駄を避けるべく, CNIが共通のインターフェイスを定める  元はと言えば rkt 用に考えられたものらしい   CNI plugins  CNIに準拠したpluginはたくさんあり, READMEにも書いてある Golangで書かれたものは多いが, CNIは言語については規定していないため何で書いても良い  CNI仕様  SPEC.mdに仕様が記されている CNI からすれば container = Linux network namespace (netns) (runtimeによりnetnsを作る単位が異なるが, CNIとしてはcontainer = network namespaceとみなす)  そもそも, Linux containerから見えるネットワークインターフェイスは コンテナ用のnetns内のinterfaceでしかない  &amp;ldquo;network&amp;rdquo; ごとに, どのpluginを用いるかなどの設定をCNIが定めるJSON形式で用意する  name: host内で一意な, networkを表す名前 type: そのnetworkで使うCNI pluginの実行ファイル名を指定 cniVersion: CNI specのバージョン ipam: I/FへのIPアドレス割り当てを管理するIPAM pluginの設定 dns: container(netns)に対するDNSを設定する その他, plugin固有の設定 (e.</description>
    </item>
    
    <item>
      <title>git-annex によるファイル管理を試した</title>
      <link>https://ntoofu.github.io/blog/post/git-annex/</link>
      <pubDate>Mon, 07 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://ntoofu.github.io/blog/post/git-annex/</guid>
      <description>LWN.netの記事 で git-annex について知ったのでちょっと試してみた。
git-annex  LWN.netの記事もそうであるように, Gitで巨大なファイルを管理する際に git-lfsと並んで名前があがることがある 大きなファイルに限らず, ファイルの実体とバージョン管理を分離することができる ユースケースについては git-annex の説明を 読むのがわかり易い  例1: バックアップ・アーカイブ用途 例2: キャッシュ的な用途（クラウドストレージにあるデータをリムーバブルメディアに持ってきて 利用した後, 書き戻す）  柔軟な制御は出来る反面面倒な部分が多い  実体の置き場を複数に分散させられるので, 分散ストレージもどきっぽくできる  置き場にはクラウドストレージ等も利用可能  保持しておくべき実体のコピーの数とかも指定できる（バックアップ用途に便利） 実体が手元にあるかないか気にする必要があるが, 裏を返せば実体は必要なものだけ持つこともできる バージョン管理はgitの流儀に従って都度自分で行うか自動でやる仕組みを整える必要がある  cronジョブを仕込むとか   ホスティングサービスの対応は薄い  GitLabでは対応を打ち切られている  ちなみに haskell で書かれている模様  検証のモチベーション  デスクトップ, ラップトップ, モバイルのデータ共有に使えないか 以下のような運用を実現したい  デスクトップは常にアクセスできるNFS上のデータにアクセスするようにしておきたい NFS上のデータは適宜クラウドストレージ(Google Drive)にバックアップとしてデータをコピーしておきたい ラップトップは自宅ではNFSのデータを使いつつ, 外出先ではローカルのデータを使い, もしローカルに無い場合はクラウドストレージからデータを持ってくるようにする  ローカルの容量が小さいので, 必要なデータだけを持ってきたい  モバイル(iOS)から参照が必要なものは都度クラウドストレージから読み出しつつ, 撮った写真とかはクラウドストレージにアップロードする  gdriveなどでrsyncライクに同期を取ることはやっていたが, 気になる部分があってやめていた  同期忘れで並行して変更が発生してしまい不整合が出たら嫌 NFSを使っている関係で書き換え発生時に自動的に同期することが出来ない   試してわかったこと 公式にwalkthroughがあるので, 参考にして色々試しつつ動きを確認した。</description>
    </item>
    
    <item>
      <title>GitHub Pagesの利用を再開した</title>
      <link>https://ntoofu.github.io/blog/post/github-pages/</link>
      <pubDate>Sun, 06 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://ntoofu.github.io/blog/post/github-pages/</guid>
      <description>元々GitHub Pagesを使ってコンピュータ関連の技術系のブログ（所謂技術ブログ）を公開しようと 1年ぐらい前にやっていて、実際Jekyllで書けるようにするところまではやっていた。 が、特に何も書かずに放置してしまっていたので、整備し直すついでにHugoに乗り換えた。 （markdownのドキュメントをHugoでHTMLにして見るようにするということを業務でやっていて、 両方を触るのが面倒だったため）
大事なのは記事をちゃんと書くことで、今までは何か書くことが出来たらーという気持ちで 構えていたのが良くなかったなと思う。これ以上やることはないというぐらいに やり尽くしたり学び尽くすなどという事は大体の事柄に対してあり得ないので、 やりきってから書くつもりだと中々その時が訪れない。 最低月1つは何か書くことにする。
また、調べたことをまとめて記事にすることも当初は考えていたが、いざとなると 「自分もネットで調べて知識を得ているのだから、まとめて書いたところで新規性も無いただの劣化コピーになるだけ」 と思ってしまい書く気にならなかった。 しかし、GitHub Pagesで書いてサクッと公開するぐらいならSEO的にもそんなに人目に触れないと思うので、 本当に私的なメモを（見ようと思えば）誰にでも見れる場所に置いているだけ、という気持ちで取り組もうと思う。
しょうもない事を長々書いたが、基本的には何かしら技術に関する話題を含む記事を今後定期的に書いていきたいと思う。</description>
    </item>
    
  </channel>
</rss>